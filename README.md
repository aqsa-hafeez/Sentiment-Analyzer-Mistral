# Sentiment Analyzer (Mistral)

An AI-powered Sentiment Analysis web application that classifies input text as **Positive**, **Negative**, or **Neutral** using the **Mistral** model via **Ollama**.

Built with:

* âš¡ FastAPI (Backend API)
* ğŸ¨ Streamlit (Frontend UI)
* ğŸ§  Mistral (Local LLM Inference)

---

## ğŸš€ Project Overview

This project demonstrates how to build a full-stack AI application using:

* A locally hosted Large Language Model
* A REST API backend
* An interactive web frontend

The application allows users to input a sentence and instantly receive its sentiment classification.

---

## ğŸ¯ Features

* Classifies text as:

  * âœ… Positive
  * âŒ Negative
  * âšª Neutral
* Fully local LLM (No OpenAI API required)
* Simple and clean UI
* FastAPI-based REST backend
* Streamlit-based frontend
* Easy to deploy and extend

---

## ğŸ—ï¸ Technology Stack

| Component     | Technology |
| ------------- | ---------- |
| LLM Model     | Mistral    |
| LLM Runtime   | Ollama     |
| Backend       | FastAPI    |
| Frontend      | Streamlit  |
| HTTP Requests | Requests   |
| Server        | Uvicorn    |

---

## ğŸ“‚ Project Structure

```
sentiment-analyzer-mistral/
â”‚
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/aqsa-hafeez/sentiment-analyzer-mistral.git
cd sentiment-analyzer-mistral
```

---

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
```

Activate:

**Windows**

```bash
venv\Scripts\activate
```

**Mac/Linux**

```bash
source venv/bin/activate
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

If needed:

```bash
pip install python-multipart
```

---

### 4ï¸âƒ£ Install & Pull Mistral Model (Ollama)

Make sure Ollama is installed and running.

Pull the model:

```bash
ollama pull mistral
```

---

## â–¶ï¸ Running the Application

### Step 1: Start Backend

```bash
uvicorn backend.main:app --reload
```

Backend will run at:

```
http://127.0.0.1:8000
```

You can verify using:

```
http://127.0.0.1:8000/docs
```

---

### Step 2: Start Frontend

Open a new terminal (do not stop backend):

```bash
streamlit run frontend/app.py
```

The browser will open automatically.

---

## ğŸ§ª Example Test Inputs

### Positive

```
I absolutely love this product!
```

### Negative

```
This is the worst experience ever.
```

### Neutral

```
The meeting is scheduled for tomorrow.
```

---

## ğŸ”„ How It Works

1. User enters text in Streamlit UI
2. Frontend sends request to FastAPI backend
3. Backend sends prompt to Mistral model via Ollama
4. Model returns sentiment
5. Result is displayed in the UI

Architecture Flow:

```
Streamlit â†’ FastAPI â†’ Ollama â†’ Mistral â†’ Response â†’ Streamlit
```

---
